{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SoMaJo in c:\\users\\vignir\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.4.3)\n",
      "Requirement already satisfied: regex>=2019.02.18 in c:\\users\\vignir\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from SoMaJo) (2025.7.34)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\Vignir\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U SoMaJo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\vignir\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\vignir\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn) (2.2.6)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\vignir\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn) (2.3.0)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\vignir\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn) (3.10.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\vignir\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\vignir\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\vignir\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.59.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\vignir\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\vignir\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\vignir\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vignir\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\vignir\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vignir\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vignir\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vignir\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\Vignir\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:22:44.240161Z",
     "start_time": "2025-08-17T14:22:43.607719Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:22:44.926002Z",
     "start_time": "2025-08-17T14:22:44.437997Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:22:44.944917Z",
     "start_time": "2025-08-17T14:22:44.940299Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:22:45.244693Z",
     "start_time": "2025-08-17T14:22:45.112752Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/cefr_leveled_texts.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:22:45.583680Z",
     "start_time": "2025-08-17T14:22:45.564410Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove the Unicode BOM Character (\\ufeff) manually\n",
    "df['text'] = df['text'].str.replace('\\ufeff', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:22:46.206522Z",
     "start_time": "2025-08-17T14:22:46.190747Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove the advertisement, business card, dictionary entry and menus from A1 level, which can't be correctly tokenized\n",
    "df = df.drop(index=[1041, 1064, 1068, 1113, 1117, 1223]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:22:48.633538Z",
     "start_time": "2025-08-17T14:22:48.590507Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi!\\r\\nI've been meaning to write for ages and...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was not so much how hard people found the c...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Keith recently came back from a trip to Chicag...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Griffith Observatory is a planetarium, and...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-LRB- The Hollywood Reporter -RRB- It's offici...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  Hi!\\r\\nI've been meaning to write for ages and...    B2\n",
       "1  It was not so much how hard people found the c...    B2\n",
       "2  Keith recently came back from a trip to Chicag...    B2\n",
       "3  The Griffith Observatory is a planetarium, and...    B2\n",
       "4  -LRB- The Hollywood Reporter -RRB- It's offici...    B2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:21:38.309339Z",
     "start_time": "2025-08-17T14:21:38.300132Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "B2    286\n",
       "A1    282\n",
       "A2    272\n",
       "C1    241\n",
       "B1    205\n",
       "C2    202\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the text\n",
    "The following program parts are generated by AI.\n",
    "\n",
    "It aims to replace non ASCII characters with ASCII equivalents so that the quality of tagging can be guaranteed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_and_summarize_non_ascii(df):\n",
    "    \"\"\"\n",
    "    Detects non-ASCII characters, flags them per column, and returns a set of all unique non-ASCII chars.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "\n",
    "    Returns:\n",
    "        set of unique non-ASCII characters\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    non_ascii_chars = set()\n",
    "\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        # New column name for detected non-ASCII characters\n",
    "        new_col = f\"{col}_non_ascii_chars\"\n",
    "\n",
    "        # Extract non-ASCII characters (if any)\n",
    "        df[new_col] = df[col].apply(\n",
    "            lambda x: ''.join([c for c in str(x) if ord(c) > 127])\n",
    "            if pd.notna(x) else None\n",
    "        )\n",
    "\n",
    "        # Update the global set of non-ASCII characters\n",
    "        unique_chars_in_col = set(''.join(df[new_col].dropna().astype(str)))\n",
    "        non_ascii_chars.update(unique_chars_in_col)\n",
    "\n",
    "        # Print column-specific issues\n",
    "        non_ascii_rows = df[df[new_col].str.len() > 0]\n",
    "        if not non_ascii_rows.empty:\n",
    "            print(f\"⚠️ Column '{col}' has {len(non_ascii_rows)} rows with non-ASCII characters.\")\n",
    "\n",
    "    # Print global summary\n",
    "    if non_ascii_chars:\n",
    "        print(\"\\n=== Unique non-ASCII characters found ===\")\n",
    "        for char in sorted(non_ascii_chars, key=ord):\n",
    "            print(f\"Character: '{char}' | Unicode: U+{ord(char):04X}\")\n",
    "    else:\n",
    "        print(\"✅ No non-ASCII characters found.\")\n",
    "\n",
    "    return non_ascii_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "STANDARDIZATION_MAP = {\n",
    "    # Whitespace and invisible characters\n",
    "    '\\u00A0': ' ',   # Non-breaking space → regular space\n",
    "\n",
    "    # Punctuation and symbols\n",
    "    '¡': '!',        # Inverted exclamation → regular !\n",
    "    '¢': 'c',        # Cent → 'c'\n",
    "    '£': 'GBP',      # Pound → 'GBP'\n",
    "    '§': 'sect',     # Section symbol → 'sect'\n",
    "    '©': '(copyright)',\n",
    "    '°': 'deg',      # Degree → 'deg'\n",
    "    '×': '*',        # Multiplication → '*'\n",
    "    '•': '-',        # Bullet → hyphen\n",
    "    '…': '...',      # Ellipsis → '...'\n",
    "\n",
    "    # Diacritics (normalize to ASCII closest equivalents)\n",
    "    'Á': 'A', 'Â': 'A', 'Ã': 'A', 'É': 'E', 'Ó': 'O', 'Ú': 'U',\n",
    "    'á': 'a', 'â': 'a', 'ã': 'a', 'ä': 'a', 'æ': 'ae', 'ç': 'c',\n",
    "    'è': 'e', 'é': 'e', 'ê': 'e', 'ë': 'e', 'í': 'i', 'î': 'i',\n",
    "    'ï': 'i', 'ñ': 'n', 'ó': 'o', 'ô': 'o', 'ö': 'o', 'ø': 'o',\n",
    "    'ú': 'u', 'ü': 'u', 'ā': 'a', 'ć': 'c', 'č': 'c', 'ł': 'l',\n",
    "    'ū': 'u',\n",
    "\n",
    "    # Scientific/math symbols (replace with descriptive tokens)\n",
    "    'µ': 'micro',    # Micro → 'micro'\n",
    "    '¼': '1/4',      # Fractions → decimals\n",
    "    '½': '1/2',\n",
    "    '−': '-',        # Minus → hyphen\n",
    "    '∼': '~',        # Tilde → ASCII ~\n",
    "    '⊙': '(circle)', # Math operators → descriptions\n",
    "    'ℝ': '(real numbers)',\n",
    "\n",
    "    # Quotes and dashes\n",
    "    '‘': \"'\", '’': \"'\", '“': '\"', '”': '\"',\n",
    "    '–': '-', '—': '--',  # En/em dashes → hyphens\n",
    "\n",
    "    # Currency symbols\n",
    "    '€': 'EUR', '₦': 'NGN', '₵': 'GHS',\n",
    "\n",
    "    # Ligatures (expand)\n",
    "    'ﬁ': 'fi', 'ﬂ': 'fl',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_text(text, standardization_map=STANDARDIZATION_MAP):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Replace each non-ASCII char using the map\n",
    "    standardized = []\n",
    "    for char in text:\n",
    "        if char in standardization_map:\n",
    "            standardized.append(standardization_map[char])\n",
    "        elif ord(char) > 127:  # Fallback for unmapped Unicode\n",
    "            standardized.append(f'(U+{ord(char):04X})')\n",
    "        else:\n",
    "            standardized.append(char)\n",
    "    return ''.join(standardized)\n",
    "\n",
    "# Apply to all string columns in a DataFrame\n",
    "def standardize_dataframe(df, standardization_map=STANDARDIZATION_MAP):\n",
    "    df_std = df.copy()\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df_std[col] = df[col].apply(lambda x: standardize_text(x, standardization_map))\n",
    "    return df_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = standardize_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No non-ASCII characters found.\n"
     ]
    }
   ],
   "source": [
    "non_ascii_chars = flag_and_summarize_non_ascii(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  # Better path handling\n",
    "\n",
    "def save_groups_to_parallel_dir(df, group_column, output_dir_name):\n",
    "    \"\"\"\n",
    "    Groups DataFrame and saves each group to CSV in a parallel directory.\n",
    "\n",
    "    Args:\n",
    "        df: pandas DataFrame\n",
    "        group_column: Column name to group by\n",
    "        parallel_dir_name: Name of the parallel directory to create\n",
    "    \"\"\"\n",
    "    # Get the current script's directory\n",
    "    script_dir = Path().parent.resolve()\n",
    "\n",
    "    # Get the parent directory (one level up)\n",
    "    parent_dir = script_dir.parent\n",
    "\n",
    "    # Create path to parallel directory\n",
    "    output_dir = parent_dir / output_dir_name\n",
    "\n",
    "    # Create directory if it doesn't exist\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Group and save files\n",
    "    groups = df.groupby(group_column)\n",
    "    for name, group in groups:\n",
    "        # Create safe filename\n",
    "        # safe_name = str(name).translate(str.maketrans('/\\\\|?*:', '_______'))\n",
    "        safe_name = str(name)\n",
    "        filepath = output_dir / f\"{safe_name}.csv\"\n",
    "        # Save to CSV\n",
    "        group.to_csv(filepath, index=False)\n",
    "\n",
    "    print(f\"Saved {len(groups)} groups to {output_dir}\")\n",
    "    return list(groups.groups.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 6 groups to C:\\Users\\Vignir\\PycharmProjects\\SUDcom\\dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A1', 'A2', 'B1', 'B2', 'C1', 'C2']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_groups_to_parallel_dir(df, group_column=\"label\", output_dir_name=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']:\n",
    "    globals()[level] = pd.read_csv(f'../dataset/{level}.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A primitive analysis of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:29:24.566834Z",
     "start_time": "2025-08-17T14:29:24.096556Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'somajo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msomajo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SoMaJo\n\u001b[0;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m SoMaJo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_PTB\u001b[39m\u001b[38;5;124m\"\u001b[39m, split_camel_case\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'somajo'"
     ]
    }
   ],
   "source": [
    "import SoMaJo\n",
    "\n",
    "tokenizer = SoMaJo(\"en_PTB\", split_camel_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:29:26.463842Z",
     "start_time": "2025-08-17T14:29:26.445514Z"
    }
   },
   "outputs": [],
   "source": [
    "def somajo_tokenize(paragraphs):\n",
    "    # paragraphs must be an iterable object\n",
    "    sentences = tokenizer.tokenize_text(paragraphs)\n",
    "\n",
    "    sentence_count = 0\n",
    "    token_each_sentence = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_count += 1\n",
    "\n",
    "        token_count = sum(1 for _ in sentence)\n",
    "        token_each_sentence.append(token_count)\n",
    "\n",
    "    token_count = sum(token_each_sentence)\n",
    "    avg_length_each_sentence = sum(token_each_sentence) / len(token_each_sentence)\n",
    "\n",
    "    return sentence_count, token_count, avg_length_each_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:30:31.508389Z",
     "start_time": "2025-08-17T14:29:29.211410Z"
    }
   },
   "outputs": [],
   "source": [
    "### Tokenizing all texts may take up to 1 min\n",
    "\n",
    "df[\"stat\"] = df[\"text\"].apply(lambda x: somajo_tokenize([x]))\n",
    "df[\"sentence_count\"] = df[\"stat\"].apply(lambda x: x[0])\n",
    "df[\"token_count\"] = df[\"stat\"].apply(lambda x: x[1])\n",
    "df[\"avg_len_each_sent\"] = df[\"stat\"].apply(lambda x: x[2]).round(2)\n",
    "df = df.drop('stat', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:30:36.573099Z",
     "start_time": "2025-08-17T14:30:36.548541Z"
    }
   },
   "outputs": [],
   "source": [
    "## include the emails\n",
    "to_check = df[ (df[\"label\"]==\"A1\") & (df[\"avg_len_each_sent\"] > 12) ]\n",
    "for index, value in to_check['text'].items():\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Row {index}\")\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:30:40.355887Z",
     "start_time": "2025-08-17T14:30:40.306799Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence_count = df.groupby('label')['sentence_count'].agg(['mean', 'std']).reset_index().round(2)\n",
    "sentence_count.attrs['title'] = \"Average number of sentences each text\"\n",
    "sentence_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:30:43.635836Z",
     "start_time": "2025-08-17T14:30:43.066261Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(sentence_count['label'], sentence_count['mean'], yerr=sentence_count['std'],\n",
    "        capsize=10, alpha=0.8, color='green')\n",
    "plt.xlabel('CEFR_level')\n",
    "plt.ylabel('Sentences each text')\n",
    "plt.title('Average number of sentences each text')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:30:46.911970Z",
     "start_time": "2025-08-17T14:30:46.864690Z"
    }
   },
   "outputs": [],
   "source": [
    "token_count = df.groupby('label')['token_count'].agg(['mean', 'std']).reset_index().round(2)\n",
    "token_count.attrs['title'] = \"Average number of tokens each text\"\n",
    "token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:30:50.898758Z",
     "start_time": "2025-08-17T14:30:50.458119Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(token_count['label'], token_count['mean'], yerr=token_count['std'],\n",
    "        capsize=10, alpha=0.8, color='green')\n",
    "plt.xlabel('CEFR_level')\n",
    "plt.ylabel('Tokens each text')\n",
    "plt.title('Average number of tokens each text')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:30:52.717587Z",
     "start_time": "2025-08-17T14:30:52.674294Z"
    }
   },
   "outputs": [],
   "source": [
    "avg_len_each_sent = df.groupby('label')['avg_len_each_sent'].agg(['mean', 'std']).reset_index().round(2)\n",
    "avg_len_each_sent.attrs['title'] = \"Average number of tokens each sentence\"\n",
    "avg_len_each_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T14:30:55.231143Z",
     "start_time": "2025-08-17T14:30:55.011662Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(avg_len_each_sent['label'], avg_len_each_sent['mean'], yerr=avg_len_each_sent['std'],\n",
    "        capsize=10, alpha=0.8, color='green')\n",
    "plt.xlabel('CEFR_level')\n",
    "plt.ylabel('Tokens each sentence')\n",
    "plt.title('Average number of tokens each sentence')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
